
# Some notes before you start:
1) This isn't meant to replace the DeepLabCut instructions - just supplement them and clear up some things that took me a long time to figure out (such as installation). It is still necessary to read over the 
DeepLabCut user guide [here](https://deeplabcut.github.io/DeepLabCut/docs/standardDeepLabCut_UserGuide.html). When looking at these instructions, I would read the DeepLabCut guide for that section first, and then look at these ones
2) Some of these functions are built under the assumptions that its being used on FreeMoCap data/recordings. One function in particular for example, will look for the `synchronized_videos` folder in a FreeMoCap recording folder. I've tried to be clear about when this happens. 
3) These are all instructions for a Windows machine. I'm not certain if they would work for other systems, particularly when it comes to the ins and outs of CUDA/TensorFlow/cudnn compatibility 

# Installing DeepLabCut
- Create an Anaconda 3.10 environment (I believe 3.10 is the latest version that can be handled by their tensorflow version, need to double-check)
- **Installation:**: `pip install deeplabcut[tf,gui]`
    - this installs both tensorflow and the deeplabcut GUI
- You could just start using DeepLabCut here, but you'll probably want to check that it is able to detect and use your GPU. You can run the script mentioned [in this section](#verifying-that-everything-is-installed-correctly) to see if your GPU is recognized by tensorflow. If not, you may need to make sure that the proper versions of CUDA and cudnn
are installed for your GPU to be recognized and used
    - The DLC docs say that their Anaconda env installation should handle installing cudnn, but that didn't seem like the case for me

## Installing CUDA, cudnn, and tensorflow
- CUDA, cudnn and tensorflow all have compatibility requirements with each other, so you have to be particular with what versions you install. Below is what worked for me.
- By default, tensorflow 2.10 gets installed with DeepLabCut
    - *note*: on Windows, tensorflow 2.10 is the last/latest version that can be installed. Any higher and you need Windows Subsystem for Linux or whatever that is-
- Given the tensorflow 2.10 installation, to match it you need
    - CUDA 11.2
    - cudnn 8.1

- Below are two methods of installation that have both seemed to work for me. In the first, you install CUDA system-wide. In the second, you install it directly into the environment. My initial thoughts on which is 
  optimal would probably depend on ease-of-use and whether you might have more recent CUDA versions and are worried about conflicts

### Option 1: Installing CUDA system-wide
- You can download CUDA 11.2 from [here](https://developer.nvidia.com/cuda-11.2.0-download-archive?target_os=Windows&target_arch=x86_64)
- You can download cudnn 8.1.1 or 8.1.0 from Nvidia's archive [here](https://developer.nvidia.com/rdp/cudnn-archive)
    - To be able to download it, I had to make a developer's account with Nvidia. I also had to specifically *opt in to the developer program* to use it. At the time of writing this I regret to say I don't remember 
    how exactly I opted in - I believe you have a chance to do so when you make the account, but I missed that, and so I google'd something like 'opt into nvidia developer program' and I found it there
- In the cudnn download folder, there are 3 folders of note, `bin`, `include`, and `lib`
- To properly install cudnn, you need to find your CUDA 11.2 folder and:
    - copy the files from `bin` in cudnn and copy/move them over to the `bin` folder of CUDA
    - copy the files from `include` in cudnn and copy/move them over to the `include` folder of CUDA
    - copy the files from the `lib\x64` folder into the `lib\x64` folder of CUDA (not into the `lib` folder, into the `x64` folder)

### Option 2: Installing CUDA and cudnn into your environment
- In your Anaconda environment, you can use condaforge to install CUDA and cudnn
- Run this command in your Anaconda prompt `conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0`
    - *Note:* I got these commands from tensorflow's installation website for Windows [here](https://www.tensorflow.org/install/pip#windows-native), take a look for more info

### Verifying that everything is installed correctly
- To verify all is well and that tensorflow recognizes your GPU, you can run the `tensorflow_verification.py` script in this repository (located [here](/tensorflow_verification.py))
- If it works successfully, you should get an output telling you how many GPU's are available to use
- If cudnn isn't installed correctly, you may see a `cudnn64_8.dll file not found` error. This is what first clued me into the fact that the `pip install` of deeplabcut wasn't installing cudnn

## Opening deeplabcut
- in your anaconda prompt, you can use `python -m deeplabcut` to bring up the DeepLabCut GUI and see if it all installed correctly
- The instructions below cover how to run DeepLabCut as headlessly as possible

# Creating a DLC config file
- with DLC working, we can make a config file.
- I created [this wrapper function](/create_project.py) in `create_project.py` to make this slightly easier
- there's a `ProjectConfig` model that takes in the necessary info to create a `config.yaml` file. 
    - fill in the project name (use underscores, not spaces, if needed) and the experimentor name
    - the `video_directory` is to the folder of initial videos you'd like to include (we can easily add more later). The `create_dlc_project` will iterate through all the videos in the folder and add them to the config file 
    - the `working_directory` is where you want to save the DLC project folder 
    - the `copy_videos_to_dlc_folder` is, as the name says, whether you want to copy the videos from the `video_directory` folder into the deeplabcut folder. If `False`, I believe it will create symbolic links to the videos. However, if you are using any sort of external drive, it will copy them over regardless. Something about symbolic links not working for that. 
- Once you run the project and create the config file, you should open the `config.yaml` folder in the newly created project folder and change the `bodyparts` to your markers, and also define any connections in `skeleton`. For example, for the wobble board, my `bodyparts` looked like:
```
bodyparts:
- roller
- mid_board
- left_board
- right_board
- front_board
- back_board
```
and my skeleton looked like:

```
skeleton:
- - left_board
  - mid_board
- - mid_board
  - right_board
- - mid_board
  - front_board
- - mid_board
  - back_board
```
# Adding New Videos
- To add new videos, I've made a [wrapper function here](/add_new_videos_to_project.py) called `add_new_videos_to_project.py`. 
- If you have a single video to add, you can just use the `deeplabcut.add_new_videos(config=config_path, videos=new_videos, copy_videos=False)` command - where `videos` is a list with the `str` path to your video.
- If you had a folder full of videos, just use the wrapper function I wrote to add all of them. You'll see the paths to the new videos show up in the `config.yaml`





## **For the next few sections (Extracting Frames until Evaluate Network), no wrapper functions are needed, so I made a [Jupyter notebook here](/run_deeplabcut.ipynb) called `run_deeplabcut.ipynb` to list out what the headless commands are**
# Extracting Frames
- Now that videos have been added, we'll want to extract frames from them to label. The `extract_frames` command will atuomatically extract frames to label from each video
- The `numframes2pick` in the `config.yaml` file is the number of frames that gets extracted. Default is 20 - I set it to 10.
- You might note that there's a `userfeedback` flag in the `extract_frames` command, but that just prompts  you to type in `y` or `n` before it proceeds with extraction for *each* video, so I wouldn't recommend it. It's annoying. 
- Extracted frames can be found in the `labeled-data` folder
- *Note:* Folders for the extracted frames are created based on video name. Because we use the same naming conventions for all our freemocap synced videos (`camera_0_synchronized` or whatever it is), every `camera_0` video that you extract videos from will go to the same folder in the deeplabcut project. I don't know yet if this is a bad thing or not important, I'm just making a note of it. 

# Labeling Data
- I cannot get this to run headlessly. The labeling Napari GUI crashes whenever I bring it up
    - to make it work I had to run the GUI (`python -m deeplabcut`) and open the labeling GUI from there
    - *update:* I ran this command from a Jupyter notebook cell for the first time today and I think it worked? 
- When labeling, the GUI will bring up folders of the extracted frame data. Choose one of them to start labeling. 
- Some notes about the Napari GUI
    - On the bottom left, you'll see the color associated with each marker. On the bottom right, you'll see a dropdown menu, the one selected is the marker currently being labeled 
    - There are different modes of how you can label the marker (located in the top right I think)
        - `sequential` and `quick` have you label every marker per frame before moving to the next frame
        - `loop` lets use label one marker on every frame before moving to the next one. This has been my preferred method so far
- When finished labeling, use `Ctrl + S` to save the data, then quit the GUI

# Check Labels
- Creates a folder of images in the `labeled_data` folder of the DLC project that shows you where the labels were placed

# Create Training Dataset
- Just run the DeepLabCut command for this 

# Train Network
- Use this to start training the network. This is the one that might take awhile. 
    - the `maxiters` flag sets the maximum iterations that it will train the project for. For the wobble board I noticed that the `loss` you see printed in the terminal plateaued after 100k iterations, so I set it to that. If you don't run it long enough, it might not minimize the loss, but if you run it too long, it might overfit the data (I think? Don't quote me on that)
    - the `saveiters` flag is after what interval of iterations you want to save the data 
    - the `displayiters` flag is how how often you want a progress update in the terminal (so every `x` amount of iterations)

# Evaluate network
- This command creates a new subdirectory with CSV's that have scores and labeled images. I have no idea how to interpret these

# Analyze Videos
- this is the guy that lets you use the model you created on other videos. I've made a wrapper function that does this and more [here](/analyze_videos.py) called `analyze_videos.py`
- the `process_recording_folder` function:
    - takes in the path to the `freemocap` recording folder whose videos you want to analyze
    - it analyzes the videos from the `synchronized_videos` folder
    - creates a folder called `dlc_data` in the `freemocap` recording folder
    - saves `csv` data from the analysis into the `dlc_data` folder
    - Uses `deeplabcut.filterpredictions` to filter the data and saves that data into the `dlc_data` folder (the filtered data will have `_filtered` in the name)
    - Creates labeled videos and saves them into the `dlc_data` folder
- the `process_session` function is for batch-processing a bunch of recordings in the same session folder. This is for example, how I applied the deeplabcut model I trained for the prosthetic data to all the prosthetic recordings 

# Refining the model
- this is where I've hit an endpoint for now, because of my issues getting the data refining to work. In theory, there's 3 things you need to do:
1) Extract outlier frames from the existing data (as seen [here](/extract_outlier_frames.py)) in `extract_outlier_frames.py`
    - the key thing here is that this function takes in the original videos and the dlc_data, so you need to give it a path to the `synchronized_videos` folder and the `dlc_data` folder to get this to work properly
2) Run the Refine Labels GUI - which is where I've hit an endpoint
3) Run the `merge_training_set` command to create a new training set, and then retrain the network

# Once you have a model you like
- Run the `analyze_videos.py` functions on all the videos you want data from
- I have methods to compile all the saved out csvs into 2d freemocap format and then reconstruct them - if you manage to get this far before I've copied them over into this repo and written then up then just let me know